{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "\t\t.builder \\\n",
    "\t\t.master(f\"local[*]\") \\\n",
    "\t\t.appName(\"myApp\") \\\n",
    "\t\t.config('spark.jars.packages', 'org.mongodb.spark:mongo-spark-connector_2.12:3.0.1') \\\n",
    "\t\t.getOrCreate()\n",
    "\n",
    "leisure = spark.read.format(\"mongo\")\\\n",
    "    .option('uri', f\"mongodb://10.4.41.93:27017/persistent.opendatabcn-leisure\") \\\n",
    "    .load().cache()\n",
    "\n",
    "income = spark.read.format(\"mongo\")\\\n",
    "    .option('uri', f\"mongodb://10.4.41.93:27017/persistent.opendatabcn-income\") \\\n",
    "    .load().cache()\n",
    "\n",
    "idealista = spark.read.format(\"mongo\")\\\n",
    "    .option('uri', f\"mongodb://10.4.41.93:27017/persistent.idealista\") \\\n",
    "    .load().cache()\n",
    "\n",
    "lookup = spark.read.format(\"mongo\")\\\n",
    "    .option('uri', f\"mongodb://10.4.41.93:27017/persistent.lookup_tables\") \\\n",
    "    .load().cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_joined_cases(row):\n",
    "    if row[1][1] is not None:\n",
    "        return (row[1][1][0], (row[1][0], row[1][1][1]))\n",
    "    else:\n",
    "        return (row[0],(row[1][0], 'unknown'))\n",
    "\n",
    "def flatten(row):\n",
    "    res=[]\n",
    "    lst = list(row)\n",
    "    res.append(lst[1][0][0][0])\n",
    "    for elem in lst[1][0][0][1]:\n",
    "        print(elem)\n",
    "        res.append(elem)\n",
    "    res.append(lst[1][0][1])\n",
    "    if lst[1][1] is not None:\n",
    "        res.append(lst[1][1][0])\n",
    "        res.append(lst[1][1][1])\n",
    "    else:\n",
    "        res.append(\"unknown\")\n",
    "        res.append(\"unknown\")\n",
    "    return tuple(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookupRDD = lookup.rdd.map(lambda t: (t['neighborhood'],(t['neighborhood_n_reconciled'], t['neighborhood_id']))).cache() \\\n",
    "     # Cache it as it will be used in multiple joins later and is very small.\n",
    "     # Decide not to remove duplicates as only 127 to 111\n",
    "    \n",
    "leisureRDD = leisure.rdd.map(lambda t: ((t['addresses_neighborhood_name'],t['secondary_filters_name']), 1)) \\\n",
    "    .reduceByKey(lambda x,y: x+y) \\\n",
    "    .coalesce(2) \\\n",
    "    .map(lambda t: (t[0][0], (t[0][1], t[1]))) \\\n",
    "    .groupByKey() \\\n",
    "    .mapValues(dict)\n",
    "    # map and reduceByKey -> count for every combination of (neigbourhood, leisure) how many (fe. (Raval, 'Biblioteques'), 20)\n",
    "    # The coalesce is very artificial in this case, but is used a 'proof of concept'. Further explanation can be found in the report.  \n",
    "    # GroupBy and MapValues -> Add them in dictionary per neighboourhood and keep it grouped by this neighborhood for later joins\n",
    "\n",
    "incomeRDD = income.rdd.map(lambda t: (t['Nom_Barri'], (t['Any'], t['Índex RFD Barcelona = 100']))) \\\n",
    "    .groupByKey() \\\n",
    "    .mapValues(dict) \\\n",
    "    # Put income in similar format as leisure with income per year in one dictionary (fe. {2007: '64.7',2008: '62.6',2009: '62.0',...})\n",
    "    # Keep grouped by neighborhood key\n",
    "\n",
    "income_join_leisure = incomeRDD \\\n",
    "      .join(leisureRDD) \\\n",
    "      .join(lookupRDD) \\\n",
    "      .map(lambda t: (t[1][-1][0], (t[1][0])))\n",
    "      # All leisure and lookup joins can happen in one stage because all grouped on same keys\n",
    "      # Not left outer join needed with lookup because count doesn't change -> every key also one key in lookup \n",
    "\n",
    "\n",
    "idealista_header = idealista.schema\n",
    "final = idealista.repartition(4).rdd.map(lambda t: (t['propertyCode'], (t['neighborhood'], t[1:]))) \\\n",
    "    .reduceByKey(lambda x,y: (x))  \\\n",
    "    .map(lambda t: (t[1][0], (t[0], t[1][1]))) \\\n",
    "    .filter(lambda t: t[0] is not None) \\\n",
    "    .leftOuterJoin(lookupRDD) \\\n",
    "    .map(lambda t: map_joined_cases(t)) \\\n",
    "    .leftOuterJoin(income_join_leisure) \\\n",
    "    .map(lambda t: flatten(t)).cache()\n",
    "    # Remove duplicates on propertyCode (21073 -> 10421)\n",
    "    # Then use neighboorhood on key (only keep when neighborhood defined: 10421 -> 6718) and join on lookup which is cached\n",
    "    # If all not joined on lookup use the name for the neighboorhood defined in the dataset themselves. Remove the None in that case.\n",
    "    # Now join on leisure and income and flatten everything such that schema can be defined on the resulting rdd\n",
    "    # + if not in income join leisure, set as \"unknown\".\n",
    "    # Cache because kpi's + Model training will use this results for multiple purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Put it in a Schema\n",
    "from pyspark.sql.types import StructType,StructField, StringType, MapType\n",
    "final_schema = [StructField('propertyCodeId',StringType(),True)]\n",
    "for idx, types in enumerate(idealista_header):\n",
    "    if idx == 0:\n",
    "        pass\n",
    "    else:\n",
    "        final_schema.append(types)\n",
    "final_schema.append(StructField('NeighborhoodId',StringType(),True))\n",
    "final_schema.append(StructField('Income',StringType(),True))\n",
    "final_schema.append(StructField('Leisure',StringType(),True))\n",
    "final_schema=StructType(final_schema)\n",
    "df = spark.createDataFrame(data = final.collect(), schema = final_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## To KPI's\n",
    "\n",
    "## KPI I: Avg. price per area for amount of theatres / parcs / libraries in the neighboorhood\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "def extract_leisure(row, leisure_type):\n",
    "    try:\n",
    "        r = re.compile(\".*{}\".format(leisure_type))\n",
    "        splt = row['Leisure'].split('=')\n",
    "        amount = list(filter(r.match, splt))[0][0:2]\n",
    "        if amount[1] == ',':\n",
    "            return leisure_type + ': ' + amount[0]\n",
    "        elif amount[0] == '{':\n",
    "            return leisure_type + ': 0'\n",
    "        return leisure_type + ': ' + amount\n",
    "    except:\n",
    "        return leisure_type + ': 0'\n",
    "\n",
    "theatres = df.rdd.map(lambda t: (extract_leisure(t, 'Teatres'),(float(t['priceByArea']),1))) \\\n",
    "    .reduceByKey(lambda x,y: ((x[0]+y[0]),(x[1]+y[1]))) \\\n",
    "    .mapValues(lambda t: t[0]/t[1])\n",
    "\n",
    "parcs = df.rdd.map(lambda t: (extract_leisure(t, 'Parcs i jardins'),(float(t['priceByArea']),1))) \\\n",
    "    .reduceByKey(lambda x,y: ((x[0]+y[0]),(x[1]+y[1]))) \\\n",
    "    .mapValues(lambda t: t[0]/t[1])\n",
    "\n",
    "libraries = df.rdd.map(lambda t: (extract_leisure(t, 'Biblioteques'),(float(t['priceByArea']),1))) \\\n",
    "    .reduceByKey(lambda x,y: ((x[0]+y[0]),(x[1]+y[1]))) \\\n",
    "    .mapValues(lambda t: t[0]/t[1])\n",
    "\n",
    "kpi1 = theatres.union(parcs).union(libraries).collect()\n",
    "kpi1.sort()\n",
    "\n",
    "fields = ['Leisure: Amount', 'Avg. Price / Area']\n",
    "import csv\n",
    "with open('kpi1.csv', 'w') as f:\n",
    "    write = csv.writer(f)\n",
    "    write.writerow(fields)\n",
    "    write.writerows(kpi1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "## KPI II: Monthly evolution of the listings Avg. price per area (based on scrape date)\n",
    "\n",
    "def extract_mmyy(row):\n",
    "    yy,mm = row['scrap_date'].split('_')[0:2]\n",
    "    return yy,mm\n",
    "    \n",
    "kpi2RDD = df.rdd.map(lambda t: (extract_mmyy(t),(float(t['priceByArea']),1))) \\\n",
    "    .reduceByKey(lambda x,y: ((x[0]+y[0]),(x[1]+y[1]))) \\\n",
    "    .mapValues(lambda t: t[0]/t[1]).cache()\n",
    "\n",
    "kpi2 = kpi2RDD.collect()\n",
    "kpi2.sort()\n",
    "\n",
    "fields = ['scraped: (year, month) ', 'Avg. Price / Area']\n",
    "import csv\n",
    "with open('kpi2.csv', 'w') as f:\n",
    "    write = csv.writer(f)\n",
    "    write.writerow(fields)\n",
    "    write.writerows(kpi2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "## KPI III: Amount of every property type per neighboorhood\n",
    "\n",
    "kpi3RDD = df.rdd.map(lambda t: ((t['neighborhood'], t['propertyType']),1)) \\\n",
    "    .reduceByKey(lambda x,y: x+y)\n",
    "\n",
    "kpi3 = kpi3RDD.collect()\n",
    "\n",
    "kpi3.sort()\n",
    "\n",
    "fields = ['(neighboorhood, propertyType)', '#listings']\n",
    "import csv\n",
    "with open('kpi3.csv', 'w') as f:\n",
    "    write = csv.writer(f)\n",
    "    write.writerow(fields)\n",
    "    write.writerows(kpi3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler, OneHotEncoder, StringIndexer,IndexToString\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.regression import GeneralizedLinearRegression\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.evaluation import RegressionEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "train,test=df.randomSplit([0.8,0.2])\n",
    "\n",
    "indexer = StringIndexer(inputCols=[\"neighborhood\", \"municipality\",\"propertyType\"], \n",
    "                        outputCols=[\"neighborhoodNum\", \"municipalityNum\",\"propertyTypeNum\",],\n",
    "                        handleInvalid='keep')\n",
    "\n",
    "ohe = OneHotEncoder(inputCols=[\"neighborhoodNum\",\"municipalityNum\"]\n",
    "                        , outputCols=[\"neighborhoodOhe\",\"municipalityOhe\"],\n",
    "                        handleInvalid='keep')\n",
    "\n",
    "vec_assembler = VectorAssembler(inputCols=['bathrooms', 'has360', 'has3DTour', 'hasLift', 'hasPlan', 'hasStaging', \n",
    "        'hasVideo', 'neighborhoodOhe', 'municipalityOhe','newDevelopment', 'newDevelopmentFinished', 'numPhotos','rooms',\n",
    "        'showAddress', 'size', 'topNewDevelopment', 'price'], outputCol='features', handleInvalid='keep')\n",
    "\n",
    "model=RandomForestClassifier(featuresCol='features', labelCol='propertyTypeNum')\n",
    "\n",
    "pipeline = Pipeline(stages=[indexer,ohe,vec_assembler,model])\n",
    "pipelineModel = pipeline.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pipelineModel.transform(test)\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"propertyTypeNum\", predictionCol=\"prediction\",metricName='weightedRecall')\n",
    "recall = evaluator.evaluate(results)\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"propertyTypeNum\", predictionCol=\"prediction\", metricName='accuracy')\n",
    "accuracy = evaluator.evaluate(results)\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"propertyTypeNum\", predictionCol=\"prediction\", metricName='f1')\n",
    "f1 = evaluator.evaluate(results)\n",
    "print(\"weighted recall = %s\" % (recall))\n",
    "print(\"accuracy = %s\" % (accuracy))\n",
    "print(\"f1 = %s\" % (f1))\n",
    "\n",
    "pipelineModel.write().overwrite().save('pipeline.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexer2 = StringIndexer(inputCols=[\"NeighborhoodId\"], \n",
    "                        outputCols=[\"NeighborhoodNum\"],\n",
    "                        handleInvalid='keep')\n",
    "\n",
    "ohe2 = OneHotEncoder(inputCols=[\"NeighborhoodNum\"]\n",
    "                        , outputCols=[\"NeighborhoodOhe\"],\n",
    "                        handleInvalid='keep')\n",
    "\n",
    "vec_assembler2 = VectorAssembler(inputCols=['NeighborhoodOhe'], outputCol='feature', handleInvalid='keep')\n",
    "\n",
    "model2=GeneralizedLinearRegression(featuresCol='feature', labelCol='price')\n",
    "\n",
    "pipeline2 = Pipeline(stages=[indexer2,ohe2,vec_assembler2,model2])\n",
    "\n",
    "pipelineModel2 = pipeline2.fit(train)\n",
    "\n",
    "results2 = pipelineModel2.transform(test)\n",
    "evaluator = RegressionEvaluator(labelCol=\"price\", predictionCol=\"prediction\",metricName='rmse')\n",
    "rmse = evaluator.evaluate(results2)\n",
    "print(\"rmse = %s\" % (rmse))\n",
    "\n",
    "pipelineModel2.write().overwrite().save('pipeline2.model')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ef9f8fcb891966bacc0decb950cf7bf7a160a7b446e410340c0803fcfb8b20cc"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('BDM')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
